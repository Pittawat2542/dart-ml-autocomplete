"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 49, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"<OOV>\", \"document_count\": 5300, \"word_counts\": \"{\\\"oov\\\": 130288}\", \"word_docs\": \"{\\\"oov\\\": 5300}\", \"index_docs\": \"{\\\"2\\\": 5300}\", \"index_word\": \"{\\\"1\\\": \\\"<OOV>\\\", \\\"2\\\": \\\"oov\\\"}\", \"word_index\": \"{\\\"<OOV>\\\": 1, \\\"oov\\\": 2}\"}}"